Instruction:

Aim:
To implement a python program for Ada Boosting.

Algorithm:
Step 1: Import Necessary Libraries
Import numpy as np.
Import pandas as pd.
Import DecisionTreeClassifier from sklearn.tree.
Import train_test_split from sklearn.model_selection.
Import accuracy_score from sklearn.metrics.

Step 2: Load and Prepare Data
Load your dataset using pd.read_csv() (e.g., df = pd.read_csv('data.csv')).
Separate features (X) and target (y).
Split the dataset into training and testing sets using train_test_split().

Step 3: Initialize Parameters
Set the number of weak classifiers n_estimators.
Initialize an array weights for instance weights, setting each weight to 1 /
number_of_samples.

Step 4: Train Weak Classifiers
Loop for n_estimators iterations:
Train a weak classifier using DecisionTreeClassifier(max_depth=1) on the training
data weighted by weights.
Predict the target values using the trained weak classifier.
Calculate the error rate err as the sum of weights of misclassified samples divided by
the sum of all weights.
Compute the classifier's weight alpha using 0.5 * np.log((1 - err) / err).
Update the weights: multiply the weights of misclassified samples by np.exp(alpha)
and the weights of correctly classified samples by np.exp(-alpha).
Normalize the weights so that they sum to 1.
Append the trained classifier and its weight to lists classifiers and alphas.

Step 5: Make Predictions
For each sample in the testing set:
Initialize a prediction score to 0.
For each trained classifier and its weight:
Add the classifier's prediction (multiplied by its weight) to the prediction score.
Take the sign of the prediction score as the final prediction.

Step 6: Evaluate the Model
Compute the accuracy of the AdaBoost model on the testing set using
accuracy_score().

Step 7: Output Results
Print or plot the final accuracy and possibly other evaluation metrics.


Program:
import pandas as pd
import numpy as np
from mlxtend.plotting import plot_decision_regions
df = pd.DataFrame()
df['X1']=[1,2,3,4,5,6,6,7,9,9]
df['X2']=[5,3,6,8,1,9,5,8,9,2]
df['label']=[1,1,0,1,0,1,0,1,0,0]
display (df)
import seaborn as sns
sns.scatterplot(x=df['X1'],y=df['X2'],hue=df['label'])

O/P

df['weights']=1/df.shape[0]
display (df)

O/p

from sklearn.tree import DecisionTreeClassifier
dt1 = DecisionTreeClassifier(max_depth=1)
x = df.iloc[:,0:2].values
y = df.iloc[:,2].values
# Step 2 - Train 1st Model
dt1.fit(x,y)
from sklearn.tree import plot_tree
plot_tree(dt1)

O/P

plot_decision_regions(x, y, clf=dt1, legend=2)

O/P

df['y pred'] = dt1.predict(x)
display (df)

O/P

def calculate_model_weight(error):
    return 0.5*np.log((1-error)/(error))
# Step - 3 Calculate model weight
alpha1 = calculate_model_weight(0.3)
# Step -4 Update weights
def update_row_weights(row,alpha):
    if row['label'] == row['y pred']:
        return row['weights']* np.exp(-alpha)
    else:
        return row['weights']* np.exp(alpha)
df['updated_weights'] = df.apply(lambda row: update_row_weights(row, alpha1), axis=1)
display (df)

O/P

df['updated_weights'].sum()

O/P

df['normalized weights']=df['updated_weights']/df['updated_weights'].sum()
display(df)

O/P

df['normalized weights'].sum()

O/P

df['cumsum_upper'] = np.cumsum(df['normalized weights'])
df['cumsum_lower']=df['cumsum_upper'] - df['normalized weights']
display(df[['X1','X2','label','weights','y pred','updated_weights','cumsum_lower','cumsum_upper']])

O/P

def create_new_dataset(df):
    indices= []
    for i in range(df.shape[0]):
        a = np.random.random()
        for index,row in df.iterrows():
            if row['cumsum_upper']>a and a>row['cumsum_lower']:
                indices.append(index)
    return indices
index_values = create_new_dataset(df)
index_values

O/P

second_df = df.iloc[index_values,[0,1,2,3]]
second_df

O/P

dt2 = DecisionTreeClassifier(max_depth=1)
x = second_df.iloc[:,0:2].values
y = second_df.iloc[:,2].values
dt2.fit(x,y)

O/P

plot_tree(dt2)

O/P

plot_decision_regions(x, y, clf=dt2, legend=2)

O/P

second_df['y_pred'] = dt2.predict(x)
second_df
alpha2 = calculate_model_weight(0.1)
display(second_df)

O/P

alpha2

O/P

# Step 4 - Update weights
def update_row_weights(row,alpha=1.09):
  if row['label'] == row['y_pred']:
    return row['weights'] * np.exp(-alpha)
  else:
    return row['weights'] * np.exp(alpha)
second_df['updated_weights'] = second_df.apply(update_row_weights,axis=1)
second_df['normalized_weights'] = second_df['updated_weights'] / second_df['updated_weights'].sum()
second_df
display(second_df)
second_df['normalized_weights'].sum()

O/P

second_df['cumsum_upper'] = np.cumsum(second_df['normalized_weights'])
second_df['cumsum_lower'] = second_df['cumsum_upper'] - second_df['normalized_weights']
second_df[['X1','X2','label','weights','y_pred','normalized_weights','cumsum_lower','cumsum_upper']]

O/P

index_values=create_new_dataset(second_df)
third_df=second_df.iloc[index_values,[0,1,2,3]]
third_df

O/P

from sklearn.tree import DecisionTreeClassifier
dt3 = DecisionTreeClassifier(max_depth=1)
x = third_df.iloc[:,0:2].values
y = third_df.iloc[:,2].values
dt3.fit(x,y)

O/P

plot_decision_regions(x, y, clf=dt3, legend=2)

O/P

third_df['y_pred'] = dt3.predict(x)
third_df
alpha3 = calculate_model_weight(0.7)
alpha3

O/P

print(alpha1,alpha2,alpha3)

O/P

query = np.array([1,5]).reshape(1,2)
dt1.predict(query)

O/P

dt2.predict(query)

O/P

dt3.predict(query)

O/P

alpha1*1 + alpha2*(1) + alpha3*(1)

O/P

np.sign(1.09)

O/P

query = np.array([9,9]).reshape(1,2)
dt1.predict(query)

O/P

dt2.predict(query)

O/P

dt3.predict(query)

O/P

alpha1*(1) + alpha2*(-1) + alpha3*(-1)

O/P

np.sign(-0.25)

O/P
--------------------------------------------------------------------------------------------------------------------------------------------------------------------




Instruction:
Aim:
To implement a python program using the gradient boosting model.

Algorithm:

Step 1: Import Necessary Libraries
Import numpy as np.
Import pandas as pd.
Import train_test_split from sklearn.model_selection.
Import DecisionTreeRegressor from sklearn.tree.
Import mean_squared_error from sklearn.metrics.

Step 2: Prepare the Data
Load your dataset into a DataFrame using pd.read_csv('your_dataset.csv').
Split the dataset into features (X) and target (y).
Use train_test_split to split the data into training and testing sets.

Step 3: Initialize Parameters
Set the number of boosting rounds (e.g., n_estimators = 100).
Set the learning rate (e.g., learning_rate = 0.1).
Initialize an empty list to store the weak learners (decision trees).
Initialize an empty list to store the learning rates for each round.

Step 4: Initialize the Base Model
Compute the initial prediction as the mean of the target values (e.g., F0 = np.mean(y_train)).
Initialize the predictions to the base model's prediction (e.g., F
= np.full(y_train.shape, F0)).

Step 5: Iterate Over Boosting Rounds
For each boosting round:
Compute the pseudo-residuals (negative gradient of the loss function) (e.g., residuals
= y_train - F).
Fit a decision tree to the pseudo-residuals.
Make predictions using the fitted tree (e.g., tree_predictions = tree.predict(X_train)).
Update the predictions by adding the learning rate multiplied by the tree predictions
(e.g., F += learning_rate * tree_predictions).
Append the fitted tree and the learning rate to their respective lists.

Step 6: Make Predictions on Test Data
Initialize the test predictions with the base model's prediction (e.g., F_test =
np.full(y_test.shape, F0)).
For each fitted tree and its learning rate:
Make predictions on the test data using the fitted tree.
Update the test predictions by adding the learning rate multiplied by the tree predictions.

Step 7: Evaluate the Model
Compute the mean squared error on the training data.
Compute the mean squared error on the test data.

Program:

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
np.random.seed(42)
X = np.random.rand(100, 1) - 0.5
y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)
df = pd.DataFrame()
df['X'] = X.reshape(100)
df['y'] = y
df

O/P

plt.scatter(df['X'],df['y'])
plt.title('X vs y')

O/P

df['pred1'] = df['y'].mean()
df

O/P

df['res1'] = df['y'] - df['pred1']
df

O/P

plt.scatter(df['X'],df['y'])
plt.plot(df['X'],df['pred1'],color='red')

O/P

from sklearn.tree import DecisionTreeRegressor
tree1 = DecisionTreeRegressor(max_leaf_nodes=8)
tree1.fit(df['X'].values.reshape(100,1),df['res1'].values)
DecisionTreeRegressor(max_leaf_nodes=8)
from sklearn.tree import plot_tree
plot_tree(tree1)
plt.show()

O/P

X_test=np.linspace(-0.5, 0.5, 500)
y_pred=0.265458 + tree1.predict(X_test.reshape(500, 1))
plt.figure(figsize=(14,4))
plt.subplot(121)
plt.plot(X_test, y_pred, linewidth=2, color='red')
plt.scatter(df['X'], df['y'])

O/P

df['pred2'] = 0.265458 + tree1.predict(df['X'].values.reshape(100,1))
df

O/P

df['res2'] = df['y'] - df['pred2']
df

O/P

from sklearn.tree import DecisionTreeRegressor
tree2 = DecisionTreeRegressor(max_leaf_nodes=8)
tree2.fit(df['X'].values.reshape(100,1),df['res2'].values)
y_pred = df['pred1'].iloc[0] + tree1.predict(X_test.reshape(-1,1)) + tree2.predict(X_test.reshape(-1,1))
plt.figure(figsize=(14,4))
plt.subplot(121)
plt.plot(X_test, y_pred, linewidth=2, color='red')
plt.scatter(df['X'], df['y'])
plt.title('X vs y')

O/P

def gradient_boost(X,y,number,lr,count=1,regs=[],foo=None):
    if number == 0:
        return
    else:
        # do gradient boosting
        if count > 1:
            y = y - regs[-1].predict(X)
        else:
            foo = y
        tree_reg = DecisionTreeRegressor(max_depth=5, random_state=42)
        tree_reg.fit(X, y)
        regs.append(tree_reg)
        x1 = np.linspace(-0.5, 0.5, 500)
        y_pred = sum(lr * regressor.predict(x1.reshape(-1, 1)) for regressor in regs)
        print(number)
        plt.figure()
        plt.plot(x1, y_pred, linewidth=2)
        plt.plot(X[:, 0], foo,"r")
        plt.show()
        gradient_boost(X,y,number-1,lr,count+1,regs,foo=foo)
np.random.seed(42)
X = np.random.rand(100, 1) - 0.5
y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)
gradient_boost(X,y,5,lr=1)

O/P
